---
layout: post
title: scrapy
subtitle: 实例
author: zz
date: 2020-03-21
head-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    scrapy
    python
    爬虫
    实例
---

>emmmmm

爬取起点网的一些数据

### 对爬虫文件hotbook.py的处理

    import scrapy
    from qidian_hot.items import QidianHotItem # 注释


    class HotbookSpider(scrapy.Spider):
        name = 'hotbook'
        allowed_domains = ['https://www.qidian.com']
        start_urls = ['https://www.qidian.com/book/strongrec']

        def parse(self, response):

            # 解析出文本
            book_forms = response.xpath("/html/body/div/ul/li/div/ul/li/a[1]/text()[2]").extract()
            book_titles = response.xpath("/html/body/div/ul/li/div/ul/li/strong/a/text()").extract()
            book_authors = response.xpath("/html/body/div/ul/li/div/ul/li/a[2]/text()").extract()

            for one in zip(book_forms, book_titles, book_authors): 
            # zip()是按元组来的，如果参数里面有个iterable有重复数据会被过滤

                one = list(one)
                item = QidianHotItem()
                item["book_form"] = one[0]
                item["book_title"] = one[1]
                item["book_author"] = one[2]

                yield item

注释1的意思是 要**从items.py文件中导出相应的Item类**，item.py中相应的Item类要实例化Field()对象，例如在本例中要有——*book_form = scrapy.Field()*。

> yield要和for连用

爬虫的开始在`start_urls`，会自动调用`start_requests(self)`对start_urls中的url进行下载。如果不想定义`start_url`，可以重写(overwrite)`start_requests(self)`方法，例子如下：

    def start_requests(self):
        url = "http://www.bilibili.com"
        yield Request(url, callback=self.parse)

*记住要指定对应的回调函数*

如果想要在爬取多页数据可以在原来的爬虫文件的解析函数中增加爬取下一页的代码：

    import scrapy
    from qidian_hot.items import QidianHotItem # 注释

    class HotbookSpider(scrapy.Spider):
        name = 'hotbook'
        allowed_domains = ['https://www.qidian.com']
        start_urls = ['https://www.qidian.com/book/strongrec']
        current_page = 1  # 创建初始页

        def parse(self, response):

            # 解析出文本
            book_forms = response.xpath("/html/body/div/ul/li/div/ul/li/a[1]/text()[2]").extract()
            book_titles = response.xpath("/html/body/div/ul/li/div/ul/li/strong/a/text()").extract()
            book_authors = response.xpath("/html/body/div/ul/li/div/ul/li/a[2]/text()").extract()

            for one in zip(book_forms, book_titles, book_authors): 
            # zip()是按元组来的，如果参数里面有个iterable有重复数据会被过滤

                one = list(one)
                item = QidianHotItem()
                item["book_form"] = one[0]
                item["book_title"] = one[1]
                item["book_author"] = one[2]

                yield item  
            
            self.current_page += 1
            if self.current_page <= 20:
                # 创建下一页的url
                next_url = "http://ttps://www.qidian.com/book/strongrec/{}".format(self.current_page)
                # 请求下一页，返回对应的Response给self.parse()
                yield Request(next_url, callback=self.parse)

### items.py文件实例

    import scrapy

    class QidianHotItem(scrapy.Item):
        book_form = scrapy.Field()
        book_title = scrapy.Field()
        book_author = scrapy.Field()

继承于scrapy.Item类

> 还有另一种可以自定义Item键的类——ItemLoader类，可以在爬虫.py中的解析函数里面进行：

    from scrapy.loader import ItemLoader

        def parse(self, response):

        book_forms = response.xpath("/html/body/div/ul/li/div/ul/li/a[1]/text()[2]").extract()
        book_titles = response.xpath("/html/body/div/ul/li/div/ul/li/strong/a/text()").extract()

        for infos in zip(book_forms, book_titles):
            novel = ItemLoader(Item=QidianHotItem())  # 实例化ItemLoader对象
            novel.add_value("book_forms", infos[0])
            novel.add_value("book_titles", infos[1])
            yield novel.load_item()
在items.py中还可以通过输入处理器(input_processor)和输出处理器(output_processor)来对Field字段进行一定的处理

    import scrapy
    from scrapy.loader.processors import TakeFirst
    
    # 定义一个转换函数
    def  form_convert(form):
        if form[0] = "历史":
            return "History"

    class QidianHotItem(scrapy.Item):
        # TakeFirst()可以提取列表里面的第一个非空数据
        book_author = scrapy.Field(output_processor=TakeFirst())
        book_form = scrapy.Field(input_processor=form_convert(), output_processor=TakeFirst())

### Item Pipeline

Pipeline可以用来：

1. 清理数据

2. 验证数据的有效性

3. 查重和丢弃

4. 数据自定义格式保存对文件

5. 数据保存到数据库

常用的几个不可重命名的方法：
`open_spider(self, spider)`一般用来打开文件或者打开数据库
`process_item(self, item, spider)`用来处理item
`close_spider(self, spider)`一般用来关闭文件和数据库
`from_crawler(cls, crawler)`通常用来访问scrapy核心组件，如setting.py

